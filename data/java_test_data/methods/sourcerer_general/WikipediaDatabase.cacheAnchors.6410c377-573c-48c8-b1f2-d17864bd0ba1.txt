public void cacheAnchors(File dir, TextProcessor tp, TIntHashSet validIds, int minLinkCount, ProgressNotifier pn) throws IOException {
    File anchorFile = new File(dir.getPath() + File.separatorChar + "anchor_summary.csv");
    File occuranceFile = new File(dir.getPath() + File.separatorChar + "anchor_occurance.csv");
    boolean cachingOccurances = occuranceFile.canRead();
    cachedAnchors = new THashMap<String, CachedAnchor>();
    BufferedReader input = new BufferedReader(new InputStreamReader(new FileInputStream(anchorFile), "UTF-8"));
    if (pn == null)
        pn = new ProgressNotifier(1);
    if (cachingOccurances)
        pn.startTask(anchorFile.length() + occuranceFile.length(), "caching anchors");
    else
        pn.startTask(anchorFile.length(), "caching anchors");
    long bytesRead = 0;
    String line;
    while ((line = input.readLine()) != null) {
        bytesRead = bytesRead + line.length();
        int sep = line.lastIndexOf(',');
        String anchor = line.substring(1, sep - 1);
        String data = line.substring(sep + 2, line.length() - 1);
        // System.out.println(anchor + " -> " + data) ;
        String[] temp = data.split(";");
        Vector<int[]> senses = new Vector<int[]>();
        for (String t : temp) {
            String[] values = t.split(":");
            if (values.length != 3)
                throw new IOException("data files are obsolete. Please run the 'patchWikipediaData.pl' script (in the extraction directory).");
            int[] sense = new int[3];
            sense[0] = new Integer(values[0]);
            sense[1] = new Integer(values[1]);
            sense[2] = new Integer(values[2]);
            // cache this if every id is valid, or if this id is valid and anchor is referred to enough times or mirrored by a title or redirect.
            if ((validIds == null || validIds.contains(sense[0])) && (sense[1] > minLinkCount || sense[2] != 0)) {
                senses.add(sense);
            }
        }
        if (senses.size() > 0) {
            if (tp != null)
                anchor = tp.processText(anchor);
            CachedAnchor ca = cachedAnchors.get(anchor);
            if (ca == null) {
                ca = new CachedAnchor(senses);
                cachedAnchors.put(anchor, ca);
            } else {
                ca.addSenses(senses);
            }
        }
        pn.update(bytesRead);
    }
    input.close();
    if (occuranceFile.canRead()) {
        input = new BufferedReader(new InputStreamReader(new FileInputStream(occuranceFile), "UTF-8"));
        while ((line = input.readLine()) != null) {
            bytesRead = bytesRead + line.length();
            int sep2 = line.lastIndexOf(',');
            int sep1 = line.lastIndexOf(',', sep2 - 1);
            String ngram = line.substring(1, sep1 - 1);
            // int linkCount = new Integer(line.substring(sep1+1, sep2)) ;
            int occCount = new Integer(line.substring(sep2 + 1));
            if (tp != null)
                ngram = tp.processText(ngram);
            // if we are doing morphological processing, then we need to resolve collisions
            CachedAnchor ca = cachedAnchors.get(ngram);
            if (ca != null)
                ca.addOccCount(occCount);
            pn.update(bytesRead);
        }
        input.close();
    }
    this.cachedProcessor = tp;
}
