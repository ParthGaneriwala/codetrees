public static void main(String[] args) throws DatabaseException {
    final MemoryMXBean memoryBean = ManagementFactory.getMemoryMXBean();
    MemoryUsage memuse;
    OptionParser optParser = new OptionParser(Options.class);
    final Options opts = (Options) optParser.parse(args, true);
    if (opts.meminfo) {
        memuse = memoryBean.getHeapMemoryUsage();
        System.out.printf("MEMUSE: initial: %dM/%dM\n", memuse.getUsed() / 1048576, memuse.getMax() / 1048576);
    }
    Experiment.initialize(opts.config);
    final Experiment experiment = Experiment.getInstance();
    if (opts.meminfo) {
        memuse = memoryBean.getHeapMemoryUsage();
        System.out.printf("MEMUSE: after Experiment.initialize(): %dM/%dM\n", memuse.getUsed() / 1048576, memuse.getMax() / 1048576);
    }
    JobManager.initialize(opts.jobs);
    Thread thread = new Thread(JobManager.getInstance(), "Job Manager");
    thread.setDaemon(true);
    thread.start();
    experiment.buildPrefixes();
    experiment.buildWordPrefixes();
    DecoderCompactProbTree emptyTree = DecoderCompactProbTree.constructEmptyTree(Experiment.getInstance().getHFT().getTree());
    DecoderCompactProbTree.setEmptyTree(emptyTree);
    Environment env = BDBProbTreeStorage.createEnvironment("data", false);
    final LanguageModel lm = experiment.getLM(opts.lm);
    final BDBProbTreeStorage storage = new BDBProbTreeStorage(env);
    storage.open(lm.getId(), lm.getIdNum(), false);
    final Collection<FactorTuple> allOvertFactors = experiment.getTupleDescription().getAllOvertFactors().keySet();
    final double[] probs = new double[opts.runs];
    class Test implements Runnable {

        int num;

        public Test(int num) {
            this.num = num;
        }

        public void run() {
            while (true) {
                BinaryTree<HistoryTreePayload> tree = lm.getHistoryTree();
                while (!tree.isLeaf()) {
                    if (Math.random() < 0.5) {
                        tree = tree.getLeft();
                    } else {
                        tree = tree.getRight();
                    }
                }
                if (tree.getPayload().isBackoff) {
                    // ignore backoff clusters
                    continue;
                }
                LinkedList<Pair<HashMap<FactorTuple, OnDiskCompactProbTree>, Double>> distributions = new LinkedList<Pair<HashMap<FactorTuple, OnDiskCompactProbTree>, Double>>();
                double lambda = 1.0;
                BinaryTree<HistoryTreePayload> leaf = tree;
                while (tree != null) {
                    int clusterid = tree.getPayload().clusterid;
                    HashMap<FactorTuple, OnDiskCompactProbTree> dist = new HashMap<FactorTuple, OnDiskCompactProbTree>();
                    for (FactorTuple word : allOvertFactors) {
                        OnDiskCompactProbTree probTree = storage.getProbTree(lm.getIdNum(), clusterid, word);
                        if (probTree != null) {
                            dist.put(word, probTree);
                        }
                    }
                    double scale = lambda * tree.getPayload().lambda;
                    distributions.add(new Pair<HashMap<FactorTuple, OnDiskCompactProbTree>, Double>(dist, scale));
                    lambda *= 1.0 - tree.getPayload().lambda;
                    tree = tree.getParent();
                }
                double totalProb = 0.0;
                for (FactorTuple overtFactors : allOvertFactors) {
                    ArrayList<OnDiskCompactProbTree> trees = new ArrayList<OnDiskCompactProbTree>(distributions.size());
                    for (Pair<HashMap<FactorTuple, OnDiskCompactProbTree>, Double> pair : distributions) {
                        HashMap<FactorTuple, OnDiskCompactProbTree> dist = pair.getFirst();
                        double scale = pair.getSecond();
                        OnDiskCompactProbTree probTree = dist.get(overtFactors);
                        if (probTree != null) {
                            probTree.scale(scale);
                            trees.add(probTree);
                        }
                    }
                    OnDiskCompactProbTree probTree = OnDiskCompactProbTree.merge(trees);
                    if (probTree == null) {
                    // FactorTuple tuple = experiment.getTupleDescription().createTuple();
                    // tuple.setOvertValues(overtFactors.factors);
                    // System.out.printf("zero prob for %s\n", tuple.toStringNoNull());
                    } else {
                        double prob = probTree.getTotalProb();
                        totalProb += prob;
                    }
                }
                probs[num] = totalProb;
                if (!ProbMath.approxEqual(probs[num], 1.0)) {
                    System.out.printf("[%d] prob = %g\n", leaf.getPayload().clusterid, probs[num]);
                }
                return;
            }
        }
    }
    JobManager manager = JobManager.getInstance();
    JobGroup group = manager.createJobGroup("test group");
    for (int i = 0; i < opts.runs; ++i) {
        Job job = new Job(new Test(i), "test");
        manager.addJob(group, job);
    }
    group.join();
    double min = 100;
    double max = 0;
    double sum = 0;
    for (double prob : probs) {
        if (prob < min)
            min = prob;
        if (prob > max)
            max = prob;
        sum += prob;
    }
    System.out.printf("Min: %e, Max: %e, Avg: %e\n", min, max, sum / probs.length);
}
