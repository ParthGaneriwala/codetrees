protected void tokenizeInstance(Instance instance, boolean updateDictionary) {
    if (m_inputVector == null) {
        m_inputVector = new LinkedHashMap<String, Count>();
    } else {
        m_inputVector.clear();
    }
    for (int i = 0; i < instance.numAttributes(); i++) {
        if (instance.attribute(i).isString() && !instance.isMissing(i)) {
            m_tokenizer.tokenize(instance.stringValue(i));
            while (m_tokenizer.hasMoreElements()) {
                String word = m_tokenizer.nextElement();
                if (m_lowercaseTokens) {
                    word = word.toLowerCase();
                }
                word = m_stemmer.stem(word);
                if (m_StopwordsHandler.isStopword(word)) {
                    continue;
                }
                Count docCount = m_inputVector.get(word);
                if (docCount == null) {
                    m_inputVector.put(word, new Count(instance.weight()));
                } else {
                    docCount.m_count += instance.weight();
                }
                if (updateDictionary) {
                    Count count = m_dictionary.get(word);
                    if (count == null) {
                        m_dictionary.put(word, new Count(instance.weight()));
                    } else {
                        count.m_count += instance.weight();
                    }
                }
            }
        }
    }
    if (updateDictionary) {
        pruneDictionary(false);
    }
}
