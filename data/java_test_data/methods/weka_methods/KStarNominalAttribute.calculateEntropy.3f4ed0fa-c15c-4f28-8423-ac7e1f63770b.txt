private void calculateEntropy(double stop, KStarWrapper params) {
    int i, j, k;
    Instance train;
    double actent = 0.0, randent = 0.0;
    double pstar, tprob, psum = 0.0, minprob = 1.0;
    double actClassProb, randClassProb;
    double[][] pseudoClassProb = new double[NUM_RAND_COLS + 1][m_NumClasses];
    // init ...
    for (j = 0; j <= NUM_RAND_COLS; j++) {
        for (i = 0; i < m_NumClasses; i++) {
            pseudoClassProb[j][i] = 0.0;
        }
    }
    for (i = 0; i < m_NumInstances; i++) {
        train = m_TrainSet.instance(i);
        if (!train.isMissing(m_AttrIndex)) {
            pstar = PStar(m_Test, train, m_AttrIndex, stop);
            tprob = pstar / m_TotalCount;
            if (pstar < minprob) {
                minprob = pstar;
            }
            psum += tprob;
            // filter instances with same class value
            for (k = 0; k <= NUM_RAND_COLS; k++) {
                // instance i is assigned a random class value in colomn k;
                // colomn k = NUM_RAND_COLS contains the original mapping:
                // instance -> class vlaue
                pseudoClassProb[k][m_RandClassCols[k][i]] += tprob;
            }
        }
    }
    // with the original class value mapping (colomn NUM_RAND_COLS)
    for (j = m_NumClasses - 1; j >= 0; j--) {
        actClassProb = pseudoClassProb[NUM_RAND_COLS][j] / psum;
        if (actClassProb > 0) {
            actent -= actClassProb * Math.log(actClassProb) / LOG2;
        }
    }
    // excluding the colomn NUM_RAND_COLS
    for (k = 0; k < NUM_RAND_COLS; k++) {
        for (i = m_NumClasses - 1; i >= 0; i--) {
            randClassProb = pseudoClassProb[k][i] / psum;
            if (randClassProb > 0) {
                randent -= randClassProb * Math.log(randClassProb) / LOG2;
            }
        }
    }
    randent /= NUM_RAND_COLS;
    // return the results ... Yuk !!!
    params.actEntropy = actent;
    params.randEntropy = randent;
    params.avgProb = psum;
    params.minProb = minprob;
}
