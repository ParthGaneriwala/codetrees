@Override
public double[] distributionForInstance(Instance instance) throws Exception {
    tokenizeInstance(instance, false);
    double[] probOfClassGivenDoc = new double[m_data.numClasses()];
    double[] logDocGivenClass = new double[m_data.numClasses()];
    for (int i = 0; i < m_data.numClasses(); i++) {
        logDocGivenClass[i] += Math.log(m_probOfClass[i]);
        LinkedHashMap<String, Count> dictForClass = m_probOfWordGivenClass.get(i);
        double allWords = 0;
        // for document normalization (if in use)
        double iNorm = 0;
        double fv = 0;
        if (m_normalize) {
            for (Map.Entry<String, Count> feature : m_inputVector.entrySet()) {
                String word = feature.getKey();
                Count c = feature.getValue();
                // check the word against all the dictionaries (all classes)
                boolean ok = false;
                for (int clss = 0; clss < m_data.numClasses(); clss++) {
                    if (m_probOfWordGivenClass.get(clss).get(word) != null) {
                        ok = true;
                        break;
                    }
                }
                // (i.e. dictionary over all classes)
                if (ok) {
                    // word counts or bag-of-words?
                    fv = (m_wordFrequencies) ? c.m_count : 1.0;
                    iNorm += Math.pow(Math.abs(fv), m_lnorm);
                }
            }
            iNorm = Math.pow(iNorm, 1.0 / m_lnorm);
        }
        // System.out.println("---- " + m_inputVector.size());
        for (Map.Entry<String, Count> feature : m_inputVector.entrySet()) {
            String word = feature.getKey();
            Count dictCount = dictForClass.get(word);
            // System.out.print(word + " ");
            /*
         * if (dictCount != null) { System.out.println(dictCount.m_count); }
         * else { System.out.println("*1"); }
         */
            // check the word against all the dictionaries (all classes)
            boolean ok = false;
            for (int clss = 0; clss < m_data.numClasses(); clss++) {
                if (m_probOfWordGivenClass.get(clss).get(word) != null) {
                    ok = true;
                    break;
                }
            }
            // ignore words we haven't seen in the training data
            if (ok) {
                double freq = (m_wordFrequencies) ? feature.getValue().m_count : 1.0;
                // double freq = (feature.getValue().m_count / iNorm * m_norm);
                if (m_normalize) {
                    freq *= (m_norm / iNorm);
                }
                allWords += freq;
                if (dictCount != null) {
                    logDocGivenClass[i] += freq * Math.log(dictCount.m_count);
                } else {
                    // leplace for zero frequency
                    logDocGivenClass[i] += freq * Math.log(m_leplace);
                }
            }
        }
        if (m_wordsPerClass[i] > 0) {
            logDocGivenClass[i] -= allWords * Math.log(m_wordsPerClass[i]);
        }
    }
    double max = logDocGivenClass[Utils.maxIndex(logDocGivenClass)];
    for (int i = 0; i < m_data.numClasses(); i++) {
        probOfClassGivenDoc[i] = Math.exp(logDocGivenClass[i] - max);
    }
    Utils.normalize(probOfClassGivenDoc);
    return probOfClassGivenDoc;
}
