public double[][] distributionsForInstances(Instances insts) throws Exception {
    // default model?
    if (m_ZeroR != null) {
        double[][] preds = new double[insts.numInstances()][];
        for (int i = 0; i < preds.length; i++) {
            preds[i] = m_ZeroR.distributionForInstance(insts.instance(i));
        }
        return preds;
    }
    final Instances numericClassInsts = new Instances(m_NumericClassData);
    for (int i = 0; i < insts.numInstances(); i++) {
        numericClassInsts.add(insts.instance(i));
    }
    // Start thread pool
    ExecutorService pool = Executors.newFixedThreadPool(m_poolSize);
    // Set up result set, and chunk size
    final int chunksize = numericClassInsts.numInstances() / m_numThreads;
    Set<Future<Void>> results = new HashSet<Future<Void>>();
    double[][] preds = new double[insts.numInstances()][];
    // For each thread
    for (int j = 0; j < m_numThreads; j++) {
        // Determine batch to be processed
        final int lo = j * chunksize;
        final int hi = (j < m_numThreads - 1) ? (lo + chunksize) : numericClassInsts.numInstances();
        // Create and submit new job for each batch of instances
        Future<Void> futureT = pool.submit(new Callable<Void>() {

            @Override
            public Void call() throws Exception {
                for (int i = lo; i < hi; i++) {
                    preds[i] = processInstance(numericClassInsts.instance(i));
                }
                return null;
            }
        });
        results.add(futureT);
    }
    // Incorporate predictions
    try {
        for (Future<Void> futureT : results) {
            futureT.get();
        }
    } catch (Exception e) {
        System.out.println("Predictions could not be generated.");
        e.printStackTrace();
    }
    pool.shutdown();
    return preds;
}
